# -*- coding: utf-8 -*-
"""Telco-Customer-Churn-Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gZ8f9vabiFIh1dMDluQ7D7O0U95sQZOL
"""

#importing the required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import zscore
import sweetviz as sv
from ydata_profiling import ProfileReport
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, roc_auc_score

data = pd.read_csv('telco-customer-churn.csv')

#displays first 5 rows of dataframe
data.head()

#overall info/structure of df
data.info()

#Number of NUll values in each column
data.isnull().sum()

#identifying Numeric datatype in df
numeric_columns = data.select_dtypes(include = 'number').columns.tolist()
numeric_columns

data.describe()

#identifying categorical column
non_numeric_columns = data.select_dtypes(include = 'object').columns.tolist()
non_numeric_columns

#convert it to numeric and handles Non-numeric columns
data['TotalCharges'] = data['TotalCharges'].apply(pd.to_numeric, errors='coerce')
data

#checks misssing values
data.isnull().sum()

#filling missing values
data = data[data['TotalCharges'].notna()]
data

#converting categorical values to numeric values
data = data.drop(columns=['customerID'])
data['gender'] = data['gender'].map({'Male': 1, 'Female': 0})
data['Partner'] = data['Partner'].map({'Yes': 1, 'No': 0})
data['Dependents'] = data['Dependents'].map({'Yes': 1, 'No': 0})
data['PhoneService'] = data['PhoneService'].map({'Yes': 1, 'No': 0})
data['MultipleLines']= data['MultipleLines'].map({'Yes': 1, 'No phone service': 2, 'No': 0})
data['InternetService'] = data['InternetService'].map({'DSL': 1, 'Fiber optic': 2, 'No': 0})
data['OnlineSecurity'] = data['OnlineSecurity'].map({'Yes': 1, 'No internet service': 2, 'No': 0})
data['OnlineBackup'] = data['OnlineBackup'].map({'Yes': 1, 'No internet service': 2, 'No': 0})
data['DeviceProtection'] = data['DeviceProtection'].map({'Yes': 1, 'No internet service': 2, 'No': 0})
data['TechSupport'] = data['TechSupport'].map({'Yes': 1, 'No internet service': 2, 'No': 0})
data['StreamingTV'] = data['StreamingTV'].map({'Yes': 1, 'No internet service': 2, 'No': 0})
data['StreamingMovies'] = data['StreamingMovies'].map({'Yes': 1, 'No internet service': 2, 'No': 0})
data['PaperlessBilling'] = data['PaperlessBilling'].map({'Yes': 1, 'No': 0})
data['PaymentMethod'] = data['PaymentMethod'].map({'Electronic check': 1,'Mailed check': 2,'Bank transfer (automatic)': 3,'Credit card (automatic)': 4})
data['Churn'] = data['Churn'].map({'Yes': 1, 'No': 0})
data['Contract'] = data['Contract'].map({'Month-to-month': 0, 'One year': 1, 'Two year': 2})

data.info()

data.isnull().sum()

#Identifying outliers from specified col
numeric_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']

def outliers_zscore(data, columns, threshold=3):
    outlier_indices = {}

    for column in columns:
        z_scores = np.abs(zscore(data[column].astype(float)))
        outliers = np.where(z_scores > threshold)
        outlier_indices[column] = outliers
    return outlier_indices

#using zscore functions it prints outliers in each column
outliers_Tenure = outliers_zscore(data,['tenure'])
print(f'Outliers in Tenure: {len(outliers_Tenure)}')
outliers_MonthlyCharges = outliers_zscore(data,['MonthlyCharges'])
print(f'Outliers in MonthlyCharges: {len(outliers_MonthlyCharges)}')
outliers_TotalCharges = outliers_zscore(data,['TotalCharges'])
print(f'Outliers in TotalCharges: {len(outliers_TotalCharges)}')

#Boxplot of Churn count distribution
sns.countplot(x='Churn', data=data)
plt.xlabel('Churn')
plt.ylabel('Count')
plt.title('Churn Distribution')
plt.show()

# Box Plot of TotalCharges by Churn
plt.figure(figsize=(12, 6))
sns.boxplot(x='Churn', y='TotalCharges',data = data)
plt.title('Box Plot of Total Charges by Churn')
plt.xlabel('Churn')
plt.ylabel('TotalCharges')
plt.show()

# Scatter Plot of TotalCharges by Contract
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Contract', y='TotalCharges', hue='Churn', data=data, alpha=0.7)
plt.title('Scatter Plot of Total Charges by Contract Type')
plt.xlabel('Contract Type')
plt.ylabel('Total Charges')
plt.legend(title='Churn Status')
plt.show()

# Bar Plot of average TotalCharges by Contract
plt.figure(figsize=(10, 6))
average_total_charges = data.groupby(['Contract', 'Churn'])['TotalCharges'].mean().reset_index()
sns.barplot(x='Contract', y='TotalCharges', data=average_total_charges, palette='viridis', hue='Churn')
plt.title('Average Total Charges by Contract Type and Churn Status')
plt.xlabel('Contract Type')
plt.ylabel('Average Total Charges')
plt.legend(title='Churn')
plt.show()

# Count the number of churn and non-churn customers
churn_counts = data['Churn'].value_counts()

# Create the pie chart
plt.figure(figsize=(6, 6))
plt.pie(churn_counts, labels=['Not Churned', 'Churned'], autopct='%1.1f%%', startangle=90)
plt.title('Churn Distribution')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

#Heatmap of correlation matrix
correlation_matrix = data.corr()

plt.figure(figsize=(11, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Heatmap of Correlation Matrix')
plt.show()

#Calculates Correlation btw target variable(churn) nd all other numeric features
correlation_with_churn = data.corr()['Churn'].sort_values(ascending=False)
print(correlation_with_churn)

#Generating a SweetViz report for dataset
sv_report = sv.analyze(data)
sv_report.show_html("sv_report.html")

# Generate a Pandas Profiling report
data = pd.read_csv('telco-customer-churn.csv')
profile = ProfileReport(data, title="Customer Churn Data Profiling Report", explorative=True)

# Save the report to an HTML file
profile.to_file("pandas_profiling_report.html")

# Separate the features (X) and the target variable (y)
features = data.drop(['Churn', 'gender'],axis=1)
target = data['Churn']

# Splitting the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Display the shapes of the training and testing sets
print(f"Features (X) Training Shape: {X_train.shape}")
print(f"Features (X) Testing Shape: {X_test.shape}")
print(f"Target (y) Training Shape: {y_train.shape}")
print(f"Target (y) Testing Shape: {y_test.shape}")

# Split the dataset into training (80%) and testing (20%) sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

# Generating a SweetViz comparison report between training and test data
comparison_report = sv.compare([train_data, "Training Data"], [test_data, "Test Data"], "Churn")
comparison_report.show_html("sv_comparison_report.html")

# Encode categorical features
for column in data.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])

# Separate features and target variable
X = data.drop('Churn', axis=1)
y = data['Churn']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Apply SMOTE to the training data
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

def train_and_evaluate(models, X_train, X_test, y_train, y_test):
    results = {}
    for name, model in models.items():
        # Fit the model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Evaluate model
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        # Store results
        results[name] = {
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall,
            'F1-Score': f1
        }
    return results

models = {
    'Naive Bayes': GaussianNB(),
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42)
}

# Evaluate models on original data
results_original = train_and_evaluate(models, X_train, X_test, y_train, y_test)

# Evaluate models on SMOTE-applied data
results_smote = train_and_evaluate(models, X_train_smote, X_test, y_train_smote, y_test)

print("Results on Original Data:", results_original)
print("Results on SMOTE Data:", results_smote)

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='recall')
grid_search.fit(X_train_smote, y_train_smote)

# Best parameters and recall score
print("Best Parameters:", grid_search.best_params_)
print("Best Recall Score:", grid_search.best_score_)

# Function to plot Confusion Matrix
def plot_confusion_matrix(y_true, y_pred, model_name):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Set2", cbar=False)
    plt.title(f"{model_name} Confusion Matrix")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()

# Function to plot Precision-Recall Curve
def plot_precision_recall_curve(model, X_test, y_test, model_name):
    y_probs = model.predict_proba(X_test)[:, 1]
    precision, recall, thresholds = precision_recall_curve(y_test, y_probs)
    plt.plot(recall, precision, label=f'{model_name}')
    plt.title(f'{model_name} Precision-Recall Curve')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.legend(loc="lower left")
    plt.show()

# Plotting all visualizations for each model
for model_name, model in models.items():
    # Fit and predict on original data
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print(f"Visualizations for {model_name}")
    plot_confusion_matrix(y_test, y_pred, model_name=model_name)
    plot_precision_recall_curve(model, X_test, y_test, model_name=model_name)

    # Fit and predict on SMOTE data
    model.fit(X_train_smote, y_train_smote)
    y_pred_smote = model.predict(X_test)

    print(f"Visualizations for {model_name} (SMOTE)")
    plot_confusion_matrix(y_test, y_pred_smote, model_name=model_name + " (SMOTE)")
    plot_precision_recall_curve(model, X_test, y_test, model_name=model_name + " (SMOTE)")

# AUC for Original Data

plt.figure(figsize=(12, 8))
for model_name, model in models.items():
    model.fit(X_train, y_train)

    # Predict probabilities for the positive class
    y_proba = model.predict_proba(X_test)[:, 1]

    # Calculate ROC curve
    fpr, tpr, thresholds = roc_curve(y_test, y_proba)

    # Calculate AUC score
    auc_score = roc_auc_score(y_test, y_proba)

    # Plot ROC curve
    plt.plot(fpr, tpr, label=f'{model_name} (Original AUC = {auc_score:.2f})')

# Plotting the diagonal line (chance)
plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')

# Show the curve
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Original Data')
plt.legend(loc='lower right')
plt.grid()
plt.show()

# AUC for SMOTE Data

plt.figure(figsize=(12, 8))
for model_name, model in models.items():
    model.fit(X_train_smote, y_train_smote)

    # Predict probabilities for the positive class
    y_proba = model.predict_proba(X_test)[:, 1]

    # Calculate ROC curve
    fpr, tpr, thresholds = roc_curve(y_test, y_proba)

    # Calculate AUC score
    auc_score = roc_auc_score(y_test, y_proba)

    # Plot ROC curve
    plt.plot(fpr, tpr, label=f'{model_name} (SMOTE AUC = {auc_score:.2f})')

# Plotting the diagonal line (chance)
plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')

# Show the curve
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for SMOTE Data')
plt.legend(loc='lower right')
plt.grid()
plt.show()